{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMl7eq/Z32HUsyLpVerdaZV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["sentiment analysis"],"metadata":{"id":"6mPariReBKOs"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0wiiPBNPBJhV","executionInfo":{"status":"ok","timestamp":1690489447916,"user_tz":-330,"elapsed":20511,"user":{"displayName":"Muhammed Irfan","userId":"16452377695759350380"}},"outputId":"f0444db4-3975-4c97-e116-1a27a0093e75"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n"]}],"source":["pip install transformers"]},{"cell_type":"code","source":["from transformers import DistilRobertaTokenizer, DistilRobertaForSequenceClassification\n","import torch\n","\n","def analyze_sentiment(text):\n","    model_name = 'distilroberta-base'\n","    tokenizer = DistilRobertaTokenizer.from_pretrained(model_name)\n","    model = DistilRobertaForSequenceClassification.from_pretrained(model_name)\n","\n","    # Preprocess the text and tokenize it\n","    inputs = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n","\n","    # Make predictions\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    # Get the predicted sentiment label\n","    logits = outputs.logits\n","    probabilities = torch.softmax(logits, dim=1).tolist()[0]\n","    sentiment_labels = ['negative', 'positive']\n","    predicted_label = sentiment_labels[probabilities.index(max(probabilities))]\n","\n","    return predicted_label\n","\n","# Example usage:\n","text_to_analyze = \"I love this movie! It's so exciting and entertaining.\"\n","sentiment = analyze_sentiment(text_to_analyze)\n","print(f\"Sentiment: {sentiment}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"id":"1FohTVgOBxYK","executionInfo":{"status":"error","timestamp":1690489536694,"user_tz":-330,"elapsed":4677,"user":{"displayName":"Muhammed Irfan","userId":"16452377695759350380"}},"outputId":"c9835995-d419-4b19-cd79-19b3bf3524ce"},"execution_count":2,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-bf38a8d11ef6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistilRobertaTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistilRobertaForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0manalyze_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'distilroberta-base'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'DistilRobertaTokenizer' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["pip install xformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KC_WVAhZCU3L","executionInfo":{"status":"ok","timestamp":1690489697197,"user_tz":-330,"elapsed":21348,"user":{"displayName":"Muhammed Irfan","userId":"16452377695759350380"}},"outputId":"13ab618b-1879-4142-bd1e-7ca5c6471475"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting xformers\n","  Downloading xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl (109.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.22.4)\n","Collecting pyre-extensions==0.0.29 (from xformers)\n","  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n","Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from xformers) (2.0.1+cu118)\n","Collecting typing-inspect (from pyre-extensions==0.0.29->xformers)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.29->xformers) (4.7.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->xformers) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->xformers) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->xformers) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->xformers) (1.3.0)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.29->xformers)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: mypy-extensions, typing-inspect, pyre-extensions, xformers\n","Successfully installed mypy-extensions-1.0.0 pyre-extensions-0.0.29 typing-inspect-0.9.0 xformers-0.0.20\n"]}]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","def analyze_sentiment(text):\n","    nlp = pipeline('sentiment-analysis', model='distilroberta-base')\n","    result = nlp(text)\n","    sentiment_label = result[0]['label']\n","    return sentiment_label\n","\n","# Example usage:\n","text_to_analyze = \"I love this movie! It's so exciting and entertaining.\"\n","sentiment = analyze_sentiment(text_to_analyze)\n","print(f\"Sentiment: {sentiment}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q_LHdiPpCGvk","executionInfo":{"status":"ok","timestamp":1690489702603,"user_tz":-330,"elapsed":826,"user":{"displayName":"Muhammed Irfan","userId":"16452377695759350380"}},"outputId":"e19c8a4a-f4f3-4b40-f172-80c65d5ea525"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Sentiment: LABEL_0\n"]}]},{"cell_type":"code","source":["from transformers import RobertaTokenizer, RobertaForSequenceClassification\n","import torch\n","\n","def analyze_sentiment(text):\n","    model_name = 'distilroberta-base'\n","    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","    model = RobertaForSequenceClassification.from_pretrained(model_name)\n","    # Preprocess the text and tokenize it\n","    inputs = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n","\n","    # Make predictions\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    # Get the predicted sentiment label\n","    logits = outputs.logits\n","    probabilities = torch.softmax(logits, dim=1).tolist()[0]\n","    sentiment_labels = ['negative', 'positive']\n","    predicted_label = sentiment_labels[probabilities.index(max(probabilities))]\n","\n","    return predicted_label\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fF8u2oCMCovU","executionInfo":{"status":"ok","timestamp":1690489972024,"user_tz":-330,"elapsed":2610,"user":{"displayName":"Muhammed Irfan","userId":"16452377695759350380"}},"outputId":"1517ada7-85aa-4bd8-8d27-17629dfcd455"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Sentiment: positive\n"]}]},{"cell_type":"code","source":["def quantize_model(model, data_loader):\n","    model.eval()\n","    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n","    model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n","\n","    # Run the model on the data to trigger quantization\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            _ = model(**batch)\n","\n","    return model\n"],"metadata":{"id":"5-gubFyGD1z1","executionInfo":{"status":"ok","timestamp":1690490096869,"user_tz":-330,"elapsed":339,"user":{"displayName":"Muhammed Irfan","userId":"16452377695759350380"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["*Function  to Perform Sentiment Analysis:*"],"metadata":{"id":"bgficjN8EW-n"}},{"cell_type":"code","source":["import torch\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification\n","\n","def analyze_sentiment(text):\n","    model_name = 'distilroberta-base'\n","    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","    model = RobertaForSequenceClassification.from_pretrained(model_name)\n","\n","    # Preprocess the text and tokenize it\n","    inputs = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n","\n","    # Make predictions\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    # Get the predicted sentiment label\n","    logits = outputs.logits\n","    probabilities = torch.softmax(logits, dim=1).tolist()[0]\n","    sentiment_labels = ['negative', 'positive']\n","    predicted_label = sentiment_labels[probabilities.index(max(probabilities))]\n","\n","    return predicted_label\n"],"metadata":{"id":"YVgHlAJjEdNc","executionInfo":{"status":"ok","timestamp":1690490238452,"user_tz":-330,"elapsed":409,"user":{"displayName":"Muhammed Irfan","userId":"16452377695759350380"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["*Function to Quantize the Model:*"],"metadata":{"id":"Jdmq10ODEhmo"}},{"cell_type":"code","source":["def quantize_model(model):\n","    # Set the model to evaluation mode for quantization\n","    model.eval()\n","\n","    # Quantize the model using post-training static quantization\n","    quantized_model = torch.quantization.quantize_dynamic(\n","        model,\n","        {torch.nn.Linear},\n","        dtype=torch.qint8\n","    )\n","\n","    return quantized_model\n"],"metadata":{"id":"Za39R8hvEkEW","executionInfo":{"status":"ok","timestamp":1690492438270,"user_tz":-330,"elapsed":1029,"user":{"displayName":"Muhammed Irfan","userId":"16452377695759350380"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["*Example*"],"metadata":{"id":"p03Ihe93EpQp"}},{"cell_type":"code","source":["import time"],"metadata":{"id":"rOKn6B8bKlSk","executionInfo":{"status":"ok","timestamp":1690491846203,"user_tz":-330,"elapsed":544,"user":{"displayName":"Muhammed Irfan","userId":"16452377695759350380"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# Prepare a dummy dataset for demonstration purposes\n","dummy_dataset = [\"I love this movie!\", \"This movie is terrible!\", \"The acting was great.\"]\n","\n","# Tokenize and preprocess the dummy dataset\n","tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n","dummy_inputs = tokenizer(dummy_dataset, padding=True, truncation=True, return_tensors='pt')\n","\n","# Load the original model\n","model = RobertaForSequenceClassification.from_pretrained('distilroberta-base')\n","\n","# Measure inference speed before quantization\n","start_time = time.time()\n","with torch.no_grad():\n","    outputs = model(**dummy_inputs)  # Pass the entire inputs dictionary at once\n","end_time = time.time()\n","inference_time_before_quantization = end_time - start_time\n","\n","# Quantize the model\n","quantized_model = quantize_model(model)\n","\n","# Save the quantized model to a file\n","output_file = \"./quantized_model.pt\"\n","torch.save(quantized_model.state_dict(), output_file)\n","print(\"Quantized model has been saved.\")\n","\n","\n","# Measure inference speed after quantization\n","start_time = time.time()\n","with torch.no_grad():\n","    outputs = quantized_model(**dummy_inputs)  # Pass the entire inputs dictionary at once\n","end_time = time.time()\n","inference_time_after_quantization = end_time - start_time\n","\n","# Print the inference times\n","print(f\"Inference Time Before Quantization: {inference_time_before_quantization:.6f} seconds\")\n","print(f\"Inference Time After Quantization: {inference_time_after_quantization:.6f} seconds\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9MF_6z8dF_U9","executionInfo":{"status":"ok","timestamp":1690492879715,"user_tz":-330,"elapsed":8412,"user":{"displayName":"Muhammed Irfan","userId":"16452377695759350380"}},"outputId":"a7b1c7ed-1a5b-4f97-f5cd-f88eb234c352"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Quantized model has been saved.\n","Inference Time Before Quantization: 0.117331 seconds\n","Inference Time After Quantization: 0.052911 seconds\n"]}]},{"cell_type":"markdown","source":["ONNX convertion"],"metadata":{"id":"XNtQkI0pQEow"}},{"cell_type":"code","source":["pip install transformers\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"we4GcyGXQkh6","executionInfo":{"status":"ok","timestamp":1690493419359,"user_tz":-330,"elapsed":10832,"user":{"displayName":"Muhammed Irfan","userId":"16452377695759350380"}},"outputId":"f788f5c5-c6d1-42ac-d30a-62a670cf7647"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"]}]},{"cell_type":"code","source":["pip install --upgrade transformers\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UmvTsoYXRS7E","executionInfo":{"status":"ok","timestamp":1690493609170,"user_tz":-330,"elapsed":9237,"user":{"displayName":"Muhammed Irfan","userId":"16452377695759350380"}},"outputId":"82c3781b-c7c4-440c-b39d-0171c7888d70"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"]}]},{"cell_type":"code","source":["pip install onnx\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R5PGrZK0ST9J","executionInfo":{"status":"ok","timestamp":1690493879068,"user_tz":-330,"elapsed":13103,"user":{"displayName":"Muhammed Irfan","userId":"16452377695759350380"}},"outputId":"6669e029-9dea-45a6-8168-134a255634ce"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting onnx\n","  Downloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.22.4)\n","Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n","Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.7.1)\n","Installing collected packages: onnx\n","Successfully installed onnx-1.14.0\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification\n","\n","# Load the DistilRoberta-Base model and tokenizer\n","model = RobertaForSequenceClassification.from_pretrained('distilroberta-base')\n","tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n","\n","# Input text\n","text = \"This is an example sentence for testing.\"\n","\n","# Tokenize the input text\n","inputs = tokenizer(text, return_tensors='pt')\n","\n","# Perform inference\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","\n","# Get the logits and probabilities\n","logits = outputs.logits\n","probabilities = torch.softmax(logits, dim=1).tolist()[0]\n","\n","# Print the probabilities for each class\n","print(\"Probabilities for each class:\")\n","print(f\"Positive: {probabilities[1]:.4f}\")\n","print(f\"Negative: {probabilities[0]:.4f}\")\n","\n","# Export the model to ONNX format\n","input_names = [\"input_ids\", \"attention_mask\"]\n","output_names = [\"logits\"]\n","dummy_input = torch.zeros(1, 512, dtype=torch.long)  # Adjust the input size based on your use case\n","torch.onnx.export(model, (inputs['input_ids'], inputs['attention_mask']), \"distilroberta-base.onnx\", verbose=True, input_names=input_names, output_names=output_names)\n","\n","print(\"DistilRoberta-Base model has been exported to ONNX format.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o339rBCQQEUZ","executionInfo":{"status":"ok","timestamp":1690493916572,"user_tz":-330,"elapsed":11795,"user":{"displayName":"Muhammed Irfan","userId":"16452377695759350380"}},"outputId":"b6f458b4-24cf-4395-b0a6-827db9a04dc6"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Probabilities for each class:\n","Positive: 0.4870\n","Negative: 0.5130\n","============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n","verbose: False, log level: Level.ERROR\n","======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n","\n","DistilRoberta-Base model has been exported to ONNX format.\n"]}]},{"cell_type":"markdown","source":["**QUANTISING ONNX MODEL**"],"metadata":{"id":"hZlsOcV7T-N5"}},{"cell_type":"code","source":["pip install onnxruntime"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qIRD_E7gUDeA","executionInfo":{"status":"ok","timestamp":1690494343267,"user_tz":-330,"elapsed":16963,"user":{"displayName":"Muhammed Irfan","userId":"16452377695759350380"}},"outputId":"c78c1be7-50b4-4401-90c4-7052e68b622d"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting onnxruntime\n","  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting coloredlogs (from onnxruntime)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.5.26)\n","Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.22.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.1)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.11.1)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n","Installing collected packages: humanfriendly, coloredlogs, onnxruntime\n","Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.15.1\n"]}]},{"cell_type":"code","source":["pip install onnx-tensorrt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uRJPzpLnU0IG","executionInfo":{"status":"ok","timestamp":1690494526967,"user_tz":-330,"elapsed":2992,"user":{"displayName":"Muhammed Irfan","userId":"16452377695759350380"}},"outputId":"87e5aa42-7617-4e5c-90fc-2dcdf154de22"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement onnx-tensorrt (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for onnx-tensorrt\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"markdown","source":["quantising"],"metadata":{"id":"i9mvAzUfX9NP"}},{"cell_type":"code","source":["pip install onnxruntime-tools"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8dmYc3qrX8Rs","executionInfo":{"status":"ok","timestamp":1690495366111,"user_tz":-330,"elapsed":10464,"user":{"displayName":"Muhammed Irfan","userId":"16452377695759350380"}},"outputId":"3b6d75a2-ed11-44d6-fb5b-f2b58c366694"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting onnxruntime-tools\n","  Downloading onnxruntime_tools-1.7.0-py3-none-any.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (from onnxruntime-tools) (1.14.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnxruntime-tools) (1.22.4)\n","Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime-tools) (15.0.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from onnxruntime-tools) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from onnxruntime-tools) (9.0.0)\n","Collecting py3nvml (from onnxruntime-tools)\n","  Downloading py3nvml-0.2.7-py3-none-any.whl (55 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime-tools) (23.1)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime-tools) (10.0)\n","Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx->onnxruntime-tools) (3.20.3)\n","Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx->onnxruntime-tools) (4.7.1)\n","Collecting xmltodict (from py3nvml->onnxruntime-tools)\n","  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n","Installing collected packages: xmltodict, py3nvml, onnxruntime-tools\n","Successfully installed onnxruntime-tools-1.7.0 py3nvml-0.2.7 xmltodict-0.13.0\n"]}]},{"cell_type":"code","source":["pip install transformers torch onnx onnxruntime"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M_5i2uI-bnkl","executionInfo":{"status":"ok","timestamp":1690496321068,"user_tz":-330,"elapsed":8470,"user":{"displayName":"Muhammed Irfan","userId":"16452377695759350380"}},"outputId":"9e4830ae-ddec-4f90-8bbb-dfa7d4279a9c"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.14.0)\n","Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (1.15.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n","Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n","Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (15.0.1)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.5.26)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime) (10.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}]},{"cell_type":"code","source":["import onnx\n","from onnxruntime.quantization import quantize_dynamic, QuantType\n","\n","# Load the original ONNX model\n","onnx_model_path = \"distilroberta-base.onnx\"\n","model = onnx.load(onnx_model_path)\n","\n","# Define the output path for the quantized model\n","quantized_model_path = \"distilroberta-base-dynamic-quantized.onnx\"\n","\n","# Quantize the model dynamically\n","quantize_dynamic(onnx_model_path, quantized_model_path, weight_type=QuantType.QUInt8)\n","\n","print(f\"The model has been quantized dynamically and saved at {quantized_model_path}.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7IPiM-o7YIN4","executionInfo":{"status":"ok","timestamp":1690498638597,"user_tz":-330,"elapsed":29908,"user":{"displayName":"Muhammed Irfan","userId":"16452377695759350380"}},"outputId":"38adc9a1-d49e-4133-a50a-51de6de9df98"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Ignore MatMul due to non constant B: /[/roberta/encoder/layer.0/attention/self/MatMul]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.0/attention/self/MatMul_1]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.1/attention/self/MatMul]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.1/attention/self/MatMul_1]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.2/attention/self/MatMul]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.2/attention/self/MatMul_1]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.3/attention/self/MatMul]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.3/attention/self/MatMul_1]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.4/attention/self/MatMul]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.4/attention/self/MatMul_1]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.5/attention/self/MatMul]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.5/attention/self/MatMul_1]\n","The model has been quantized dynamically and saved at distilroberta-base-dynamic-quantized.onnx.\n"]}]},{"cell_type":"markdown","source":["testing"],"metadata":{"id":"mrWq_w21lC83"}},{"cell_type":"code","source":["from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","import torch\n","import onnxruntime\n","from onnxruntime.quantization import quantize_dynamic, QuantType\n","\n","# Load the PyTorch model and tokenizer\n","model_name = \"distilroberta-base\"\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# An example input\n","input_ids = tokenizer.encode(\"This is a sample input\", return_tensors='pt')\n","\n","# Define names and dynamic axes for inputs/outputs\n","input_names = ['input_ids', 'attention_mask']\n","output_names = ['output']\n","dynamic_axes = {'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n","                'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n","                'output': {0: 'batch_size'}}\n","\n","# Export the model\n","torch.onnx.export(model,\n","                  (input_ids, input_ids),\n","                  \"distilroberta-base.onnx\",\n","                  input_names=input_names,\n","                  output_names=output_names,\n","                  dynamic_axes=dynamic_axes)\n","\n","# Quantize the model dynamically\n","quantize_dynamic(\"distilroberta-base.onnx\",\n","                 \"distilroberta-base-dynamic-quantized.onnx\",\n","                 weight_type=QuantType.QUInt8)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HsCGeoKlle7_","executionInfo":{"status":"ok","timestamp":1690499076762,"user_tz":-330,"elapsed":42279,"user":{"displayName":"Muhammed Irfan","userId":"16452377695759350380"}},"outputId":"590480ef-9515-48ef-a353-18c7d8aa0948"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n","verbose: False, log level: Level.ERROR\n","======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n","\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.0/attention/self/MatMul]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.0/attention/self/MatMul_1]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.1/attention/self/MatMul]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.1/attention/self/MatMul_1]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.2/attention/self/MatMul]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.2/attention/self/MatMul_1]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.3/attention/self/MatMul]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.3/attention/self/MatMul_1]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.4/attention/self/MatMul]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.4/attention/self/MatMul_1]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.5/attention/self/MatMul]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.5/attention/self/MatMul_1]\n"]}]}]}